{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDPpfLXisUZV",
        "outputId": "b3dd208d-b49c-4025-8783-2e2336e69b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Successfully uninstalled tensorflow-2.17.0\n",
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.12.1)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.0)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.0)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.64.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Collecting tensorflow_addons==0.16.1\n",
            "  Downloading tensorflow_addons-0.16.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons==0.16.1) (4.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=2.7->tensorflow_addons==0.16.1) (4.12.2)\n",
            "Downloading tensorflow_addons-0.16.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.16.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.8.0\n",
        "!pip install tensorflow_addons==0.16.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWW4QMsDsUZW"
      },
      "outputs": [],
      "source": [
        "!python train.py --model_id 1 --train_dir=/content/drive/MyDrive/plant/train --val_dir=/content/drive/MyDrive/plant/val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D0q-8HSwsUZW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, Dense, LeakyReLU, Flatten, MaxPooling2D, Input\n",
        "\n",
        "# Util\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Train\n",
        "# import argparse\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# from utils import gen_maker, CustomCallback\n",
        "# from networks import model_maker\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "import tensorflow_addons as tfa\n",
        "import pickle as pkl\n",
        "\n",
        "# eval\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow_addons.optimizers import AdamW\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT CODE STARTS FOR OBJECT LOCALIZATION"
      ],
      "metadata": {
        "id": "g9b_3-v1wneS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def localization_network(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, (3, 3), activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    # Output layer to predict bounding box (4 values: x_min, y_min, x_max, y_max)\n",
        "    bbox = layers.Dense(4, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, bbox, name=\"LocalizationNetwork\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "AxvusaShweFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_classifier_with_localization(input_shape, patch_size, num_patches, projection_dim, transformer_units,\n",
        "                                            transformer_layers, model_name, num_heads, mlp_head_units, num_classes):\n",
        "    # Localization network to predict bounding box\n",
        "    localization_net = localization_network(input_shape)\n",
        "\n",
        "    # Input for the original images\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Predict bounding box\n",
        "    bbox = localization_net(inputs)\n",
        "\n",
        "    # Crop the image using the bounding box coordinates\n",
        "    def crop_and_resize(image, bbox):\n",
        "    # Unpack bbox coordinates\n",
        "      x_min, y_min, x_max, y_max = tf.unstack(bbox, axis=-1)\n",
        "\n",
        "      # Calculate offsets and target dimensions, applying per-batch processing\n",
        "      offset_height = tf.cast(tf.round(y_min * tf.cast(tf.shape(image)[1], tf.float32)), tf.int32)\n",
        "      offset_width = tf.cast(tf.round(x_min * tf.cast(tf.shape(image)[2], tf.float32)), tf.int32)\n",
        "      target_height = tf.cast(tf.round((y_max - y_min) * tf.cast(tf.shape(image)[1], tf.float32)), tf.int32)\n",
        "      target_width = tf.cast(tf.round((x_max - x_min) * tf.cast(tf.shape(image)[2], tf.float32)), tf.int32)\n",
        "\n",
        "      # Ensure batch compatibility with offsets and target dimensions\n",
        "      cropped_images = tf.map_fn(\n",
        "          lambda img_and_box: tf.image.crop_to_bounding_box(\n",
        "              img_and_box[0],\n",
        "              img_and_box[1],\n",
        "              img_and_box[2],\n",
        "              img_and_box[3],\n",
        "              img_and_box[4]\n",
        "          ),\n",
        "          elems=(image, offset_height, offset_width, target_height, target_width),\n",
        "          dtype=image.dtype\n",
        "      )\n",
        "\n",
        "      return cropped_images\n",
        "\n",
        "\n",
        "    localized_inputs = layers.Lambda(lambda x: crop_and_resize(x[0], x[1]))([inputs, bbox])\n",
        "\n",
        "    # Pass the localized image into the ViT\n",
        "    patches = Patches(patch_size)(localized_inputs)\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    for _ in range(transformer_layers):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    logits = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=logits, name=model_name)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "hYXQgJqYwnE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n2ZxAOUXsUZW"
      },
      "outputs": [],
      "source": [
        "# networks.py\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "    return x\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):  # Ensure kwargs is included here\n",
        "        super(Patches, self).__init__(**kwargs)  # Pass kwargs to the parent class\n",
        "        # super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "    def get_config(self):\n",
        "      # Add 'patch_size' to the layer config\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "          \"patch_size\": self.patch_size,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super(PatchEncoder, self).__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PatchEncoder, self).get_config()  # Call get_config from the parent class\n",
        "        config.update({\n",
        "            \"num_patches\": self.num_patches,\n",
        "            \"projection_dim\": self.projection.units,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "def create_vit_classifier(input_shape, patch_size, num_patches, projection_dim, transformer_units, transformer_layers, model_name, num_heads, mlp_head_units, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    #representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes, activation = 'softmax')(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits, name = model_name)\n",
        "    return model\n",
        "\n",
        "# def model_maker(target_size, model_id, num_classes = 3):\n",
        "#     \"\"\" This function creates a trainable model.\n",
        "#         params:\n",
        "#             target_size: tuple, size of the input image to the network\n",
        "#             model_id: integer, it can be 1 to 4\n",
        "#         returns:\n",
        "#             tensorflow trainable model.\n",
        "#     \"\"\"\n",
        "\n",
        "#     if model_id == 1:\n",
        "#         inp = Input(shape = (*target_size, 3), name = 'input_layer')\n",
        "#         cnv1 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_1')(inp)\n",
        "#         cnv2 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_2')(cnv1)\n",
        "#         mxp1 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_1')(cnv2)\n",
        "#         cnv3 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_3')(mxp1)\n",
        "#         cnv4 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_4')(cnv3)\n",
        "#         mxp2 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_2')(cnv4)\n",
        "#         fltn = Flatten(name = 'flatten_layer')(mxp2)\n",
        "#         FC1 = Dense(50, name = 'FC_1')(fltn)\n",
        "#         FC1 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_1')(FC1)\n",
        "#         FC2 = Dense(50, name = 'FC_2')(FC1)\n",
        "#         FC2 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_2')(FC2)\n",
        "#         output = Dense(num_classes, activation = 'softmax', name = 'output_layer')(FC2)\n",
        "#         model = Model(inputs = inp, outputs = output, name = 'WheatClassifier_CNN_'+str(model_id))\n",
        "#         model.summary()\n",
        "\n",
        "#     if model_id == 2:\n",
        "#         inp = Input(shape = (*target_size, 3), name = 'input_layer')\n",
        "#         cnv1 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_1')(inp)\n",
        "#         cnv2 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_2')(cnv1)\n",
        "#         mxp1 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_1')(cnv2)\n",
        "#         cnv3 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_3')(mxp1)\n",
        "#         cnv4 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_4')(cnv3)\n",
        "#         mxp2 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_2')(cnv4)\n",
        "#         cnv5 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_5')(mxp2)\n",
        "#         cnv6 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_6')(cnv5)\n",
        "#         mxp3 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_3')(cnv6)\n",
        "#         fltn = Flatten(name = 'flatten_layer')(mxp3)\n",
        "#         FC1 = Dense(50, name = 'FC_1')(fltn)\n",
        "#         FC1 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_1')(FC1)\n",
        "#         FC2 = Dense(50, name = 'FC_2')(FC1)\n",
        "#         FC2 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_2')(FC2)\n",
        "#         output = Dense(num_classes, activation = 'softmax', name = 'output_layer')(FC2)\n",
        "#         model = Model(inputs = inp, outputs = output, name = 'WheatClassifier_CNN_'+str(model_id))\n",
        "#         model.summary()\n",
        "\n",
        "#     elif model_id == 3:\n",
        "#         image_size = target_size[0]  # We'll resize input images to this size\n",
        "#         patch_size = 10  # Size of the patches to be extract from the input images\n",
        "#         num_patches = (image_size // patch_size) ** 2\n",
        "#         projection_dim = 64\n",
        "#         num_heads = 4\n",
        "#         transformer_units = [\n",
        "#             projection_dim * 2,\n",
        "#             projection_dim]  # Size of the transformer layers\n",
        "#         transformer_layers = 2\n",
        "#         mlp_head_units = [50, 50]\n",
        "#         model = create_vit_classifier((*target_size, 3),\n",
        "#                                       patch_size,\n",
        "#                                       num_patches,\n",
        "#                                       projection_dim,\n",
        "#                                       transformer_units,\n",
        "#                                       transformer_layers,\n",
        "#                                       'WheatClassifier_VIT_'+str(model_id),\n",
        "#                                       num_heads, mlp_head_units, num_classes)\n",
        "#         model.summary()\n",
        "\n",
        "#     elif model_id == 4:\n",
        "#         image_size = target_size[0]  # We'll resize input images to this size\n",
        "#         patch_size = 10  # Size of the patches to be extract from the input images\n",
        "#         num_patches = (image_size // patch_size) ** 2\n",
        "#         projection_dim = 64\n",
        "#         num_heads = 4\n",
        "#         transformer_units = [\n",
        "#             projection_dim * 2,\n",
        "#             projection_dim]  # Size of the transformer layers\n",
        "#         transformer_layers = 4\n",
        "#         mlp_head_units = [50, 50]\n",
        "#         model = create_vit_classifier((*target_size, 3),\n",
        "#                                       patch_size,\n",
        "#                                       num_patches,\n",
        "#                                       projection_dim,\n",
        "#                                       transformer_units,\n",
        "#                                       transformer_layers,\n",
        "#                                       'WheatClassifier_VIT_'+str(model_id),\n",
        "#                                       num_heads, mlp_head_units, num_classes)\n",
        "\n",
        "#         model.summary()\n",
        "\n",
        "#     elif model_id == 5:\n",
        "#         inp = Input(shape = (*target_size, 3), name = 'input_layer')\n",
        "#         cnv1 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_1')(inp)\n",
        "#         cnv2 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_2')(cnv1)\n",
        "#         mxp1 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_1')(cnv2)\n",
        "#         size_mxp1 = getattr(mxp1, 'shape')\n",
        "#         image_size =size_mxp1[1]  # We'll resize input images to this size\n",
        "#         patch_size = 10  # Size of the patches to be extract from the input images\n",
        "#         num_patches = (image_size // patch_size) ** 2\n",
        "#         projection_dim = 64\n",
        "#         num_heads = 4\n",
        "#         transformer_units = [\n",
        "#             projection_dim * 2,\n",
        "#             projection_dim]  # Size of the transformer layers\n",
        "#         transformer_layers = 1\n",
        "#         mlp_head_units = [50, 50]\n",
        "#         patches = Patches(patch_size)(mxp1)\n",
        "#         # Encode patches.\n",
        "#         encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "#         # Create multiple layers of the Transformer block.\n",
        "#         for _ in range(transformer_layers):\n",
        "#             # Layer normalization 1.\n",
        "#             x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "#             # Create a multi-head attention layer.\n",
        "#             attention_output = layers.MultiHeadAttention(\n",
        "#                 num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "#             )(x1, x1)\n",
        "#             # Skip connection 1.\n",
        "#             x2 = layers.Add()([attention_output, encoded_patches])\n",
        "#             # Layer normalization 2.\n",
        "#             x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "#             # MLP.\n",
        "#             x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "#             # Skip connection 2.\n",
        "#             encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "#         # Create a [batch_size, projection_dim] tensor.\n",
        "#         representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "\n",
        "#         fltn = Flatten(name = 'flatten_layer')(representation)\n",
        "#         FC1 = Dense(50, name = 'FC_1')(fltn)\n",
        "#         FC1 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_1')(FC1)\n",
        "#         FC2 = Dense(50, name = 'FC_2')(FC1)\n",
        "#         FC2 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_2')(FC2)\n",
        "#         output = Dense(num_classes, activation = 'softmax', name = 'output_layer')(FC2)\n",
        "#         model = Model(inputs = inp, outputs = output, name = 'WheatClassifier_CNN_'+str(model_id))\n",
        "#         model.summary()\n",
        "\n",
        "#     elif model_id == 6:\n",
        "#         inp = Input(shape = (*target_size, 3), name = 'input_layer')\n",
        "#         cnv1 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_1')(inp)\n",
        "#         cnv2 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_2')(cnv1)\n",
        "#         mxp1 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_1')(cnv2)\n",
        "#         cnv3 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_3')(mxp1)\n",
        "#         cnv4 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_4')(cnv3)\n",
        "#         mxp2 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_2')(cnv4)\n",
        "#         size_mxp2 = getattr(mxp2, 'shape')\n",
        "#         image_size =size_mxp2[1]  # We'll resize input images to this size\n",
        "#         patch_size = 5  # Size of the patches to be extract from the input images\n",
        "#         num_patches = (image_size // patch_size) ** 2\n",
        "#         print(num_patches, patch_size, image_size)\n",
        "#         projection_dim = 64\n",
        "#         num_heads = 4\n",
        "#         transformer_units = [\n",
        "#           projection_dim * 2,\n",
        "#           projection_dim]  # Size of the transformer layers\n",
        "#         transformer_layers = 2\n",
        "#         mlp_head_units = [50, 50]\n",
        "#         patches = Patches(patch_size)(mxp2)\n",
        "#       # Encode patches.\n",
        "#         encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "#       # Create multiple layers of the Transformer block.\n",
        "#         for _ in range(transformer_layers):\n",
        "#           # Layer normalization 1.\n",
        "#             x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "#           # Create a multi-head attention layer.\n",
        "#             attention_output = layers.MultiHeadAttention(\n",
        "#               num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "#             )(x1, x1)\n",
        "#           # Skip connection 1.\n",
        "#             x2 = layers.Add()([attention_output, encoded_patches])\n",
        "#           # Layer normalization 2.\n",
        "#             x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "#           # MLP.\n",
        "#             x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "#           # Skip connection 2.\n",
        "#             encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "#       # Create a [batch_size, projection_dim] tensor.\n",
        "#         representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "\n",
        "#         fltn = Flatten(name = 'flatten_layer')(representation)\n",
        "#         FC1 = Dense(50, name = 'FC_1')(fltn)\n",
        "#         FC1 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_1')(FC1)\n",
        "#         FC2 = Dense(50, name = 'FC_2')(FC1)\n",
        "#         FC2 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_2')(FC2)\n",
        "#         output = Dense(num_classes, activation = 'softmax', name = 'output_layer')(FC2)\n",
        "#         model = Model(inputs = inp, outputs = output, name = 'WheatClassifier_CNN-VIT_'+str(model_id))\n",
        "#         model.summary()\n",
        "\n",
        "#     elif model_id == 7:\n",
        "#         image_size = target_size[0]  # We'll resize input images to this size\n",
        "#         patch_size = 10  # Size of the patches to be extract from the input images\n",
        "#         num_patches = (image_size // patch_size) ** 2\n",
        "#         projection_dim = 64\n",
        "#         num_heads = 4\n",
        "#         transformer_units = [\n",
        "#           projection_dim * 2,\n",
        "#           projection_dim]  # Size of the transformer layers\n",
        "#         transformer_layers = 1\n",
        "#         mlp_head_units = [50, 50]\n",
        "#         inputs = layers.Input(shape = (*target_size, 3), name = 'input_layer')\n",
        "#       # Create patches.\n",
        "#         patches = Patches(patch_size)(inputs)\n",
        "\n",
        "#       # Encode patches.\n",
        "#         encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "#       # Create multiple layers of the Transformer block.\n",
        "#         for _ in range(transformer_layers):\n",
        "#           # Layer normalization 1.\n",
        "#             x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "#           # Create a multi-head attention layer.\n",
        "#             attention_output = layers.MultiHeadAttention(\n",
        "#               num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "#           )(x1, x1)\n",
        "#           # Skip connection 1.\n",
        "#             x2 = layers.Add()([attention_output, encoded_patches])\n",
        "#           # Layer normalization 2.\n",
        "#             x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "#           # MLP.\n",
        "#             x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "#           # Skip connection 2.\n",
        "#             encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "#       # Create a [batch_size, projection_dim] tensor.\n",
        "#         representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "#         size_rep = getattr(representation, 'shape')\n",
        "\n",
        "#         representation = tf.keras.layers.Reshape((int(size_rep[1]**0.5), int(size_rep[1]**0.5), size_rep[2]))(representation)\n",
        "#         cnv1 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_1')(representation)\n",
        "#         cnv2 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_2')(cnv1)\n",
        "#         if int(size_rep[1]**0.5)>5:\n",
        "#             mxp1 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_1')(cnv2)\n",
        "\n",
        "#             fltn = Flatten(name = 'flatten_layer')(mxp1)\n",
        "#         else:\n",
        "#             fltn = Flatten(name = 'flatten_layer')(cnv2)\n",
        "#         FC1 = Dense(50, name = 'FC_1')(fltn)\n",
        "#         FC1 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_1')(FC1)\n",
        "#         FC2 = Dense(50, name = 'FC_2')(FC1)\n",
        "#         FC2 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_2')(FC2)\n",
        "#         output = Dense(num_classes, activation = 'softmax', name = 'output_layer')(FC2)\n",
        "#         model = Model(inputs = inputs, outputs = output, name = 'WheatClassifier_CNN_'+str(1))\n",
        "#         model.summary()\n",
        "\n",
        "#     elif model_id == 8:\n",
        "#         image_size = target_size[0]  # We'll resize input images to this size\n",
        "#         patch_size = 10  # Size of the patches to be extract from the input images\n",
        "#         num_patches = (image_size // patch_size) ** 2\n",
        "#         projection_dim = 64\n",
        "#         num_heads = 4\n",
        "#         transformer_units = [\n",
        "#           projection_dim * 2,\n",
        "#           projection_dim]  # Size of the transformer layers\n",
        "#         transformer_layers = 2\n",
        "#         mlp_head_units = [50, 50]\n",
        "#         inputs = layers.Input(shape = (*target_size, 3), name = 'input_layer')\n",
        "#       # Create patches.\n",
        "#         patches = Patches(patch_size)(inputs)\n",
        "\n",
        "#       # Encode patches.\n",
        "#         encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "#       # Create multiple layers of the Transformer block.\n",
        "#         for _ in range(transformer_layers):\n",
        "#           # Layer normalization 1.\n",
        "#             x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "#           # Create a multi-head attention layer.\n",
        "#             attention_output = layers.MultiHeadAttention(\n",
        "#               num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "#             )(x1, x1)\n",
        "#           # Skip connection 1.\n",
        "#             x2 = layers.Add()([attention_output, encoded_patches])\n",
        "#           # Layer normalization 2.\n",
        "#             x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "#           # MLP.\n",
        "#             x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "#           # Skip connection 2.\n",
        "#             encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "#       # Create a [batch_size, projection_dim] tensor.\n",
        "#         representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "#         size_rep = getattr(representation, 'shape')\n",
        "\n",
        "#         representation = tf.keras.layers.Reshape((int(size_rep[1]**0.5), int(size_rep[1]**0.5), size_rep[2]))(representation)\n",
        "#         if int(size_rep[1]**0.5)==5:\n",
        "#             cnv1 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_1', padding = 'same')(representation)\n",
        "#             cnv2 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_2', padding = 'same')(cnv1)\n",
        "#             #mxp1 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_1')(cnv2)\n",
        "\n",
        "#             cnv3 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_3', padding = 'valid')(cnv2)\n",
        "#             cnv4 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_4', padding = 'valid')(cnv3)\n",
        "#             #mxp2 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_2')(cnv4)\n",
        "\n",
        "\n",
        "#             fltn = Flatten(name = 'flatten_layer')(cnv4)\n",
        "#         elif int(size_rep[1]**0.5)==10:\n",
        "#             cnv1 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_1', padding = 'valid')(representation)\n",
        "#             cnv2 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_2', padding = 'valid')(cnv1)\n",
        "#             #mxp1 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_1')(cnv2)\n",
        "\n",
        "#             cnv3 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_3', padding = 'valid')(cnv2)\n",
        "#             cnv4 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_4', padding = 'valid')(cnv3)\n",
        "#             #mxp2 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_2')(cnv4)\n",
        "\n",
        "\n",
        "#             fltn = Flatten(name = 'flatten_layer')(cnv4)\n",
        "#         else:\n",
        "#             cnv1 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_1', padding = 'valid')(representation)\n",
        "#             cnv2 = Conv2D(filters = 10, kernel_size = (3, 3), strides = (1, 1), name = 'conv_2', padding = 'valid')(cnv1)\n",
        "#             mxp1 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_1')(cnv2)\n",
        "\n",
        "#             cnv3 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_3', padding = 'valid')(mxp1)\n",
        "#             cnv4 = Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), name = 'conv_4', padding = 'valid')(cnv3)\n",
        "#             mxp2 = MaxPooling2D(pool_size = (2, 2), strides= (2, 2), name = 'maxpool_2')(cnv4)\n",
        "\n",
        "\n",
        "#             fltn = Flatten(name = 'flatten_layer')(mxp2)\n",
        "\n",
        "#         FC1 = Dense(50, name = 'FC_1')(fltn)\n",
        "#         FC1 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_1')(FC1)\n",
        "#         FC2 = Dense(50, name = 'FC_2')(FC1)\n",
        "#         FC2 = LeakyReLU(alpha = 0.3, name = 'leaky_ReLu_2')(FC2)\n",
        "#         output = Dense(num_classes, activation = 'softmax', name = 'output_layer')(FC2)\n",
        "#         model = Model(inputs = inputs, outputs = output, name = 'WheatClassifier_CNN_'+str(1))\n",
        "#         model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     return model\n",
        "def model_maker(target_size, model_id, num_classes=3):\n",
        "    image_size = target_size[0]\n",
        "    patch_size = 10\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    projection_dim = 64\n",
        "    num_heads = 4\n",
        "    transformer_units = [projection_dim * 2, projection_dim]\n",
        "    transformer_layers = 2\n",
        "    mlp_head_units = [50, 50]\n",
        "\n",
        "    model = create_vit_classifier_with_localization(\n",
        "        (*target_size, 3), patch_size, num_patches, projection_dim, transformer_units,\n",
        "        transformer_layers, 'WheatClassifier_VIT_' + str(model_id), num_heads, mlp_head_units, num_classes\n",
        "    )\n",
        "    model.summary()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdYF1QH6tCM3",
        "outputId": "205ebbb3-4126-4073-ae42-662bc71330f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bTtUAiWSsUZX"
      },
      "outputs": [],
      "source": [
        "def gen_maker(train_path, val_path, target_size=(100, 100), batch_size=16, mode='categorical'):\n",
        "    \"\"\"\n",
        "    This function creates data generators for train and validation data.\n",
        "    params:\n",
        "        train_path: path to the training data folder, string.\n",
        "        val_path: path to the validation data folder, string.\n",
        "        target_size: size of the inputs to the network, tuple.\n",
        "        batch_size: the batch size for training and validation, integer.\n",
        "        mode: classification mode, it can be either \"binary\" or \"categorical\"\n",
        "    returns:\n",
        "        train_generator: data generator for training data.\n",
        "        validation_generator: data generator for validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    train_datagen = ImageDataGenerator( rotation_range=10,\n",
        "                                    width_shift_range=0.2,\n",
        "                                    height_shift_range=0.2,\n",
        "                                    shear_range=0.1,\n",
        "                                    zoom_range=0.1,\n",
        "                                    channel_shift_range=0.0,\n",
        "                                    horizontal_flip=True,\n",
        "                                    vertical_flip=True,\n",
        "                                    rescale=1./255)\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=mode)\n",
        "\n",
        "    validation_generator = test_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        class_mode=mode)\n",
        "    return train_generator, validation_generator\n",
        "\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    This callback saves the model at the end of each epoch and calculates\n",
        "    the confusion matrix and classification report on the validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, val_gen, model_path, model_id):\n",
        "\n",
        "        super(CustomCallback, self).__init__()\n",
        "        self.val_gen = val_gen\n",
        "        self.model_path = model_path\n",
        "        self.model_id = model_id\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.model.save(self.model_path + 'epoch{}-id{}'.format(epoch,self.model_id ))\n",
        "        y_pred = self.model.predict(self.val_gen)\n",
        "        y_pred = np.squeeze(np.argmax(y_pred, axis = 1))\n",
        "        y_true = self.val_gen.classes\n",
        "        cnf = confusion_matrix(y_true, y_pred)\n",
        "        cls_report = classification_report(y_true, y_pred)\n",
        "        print('\\nclassification report:\\n', cls_report)\n",
        "        print('\\nconfusion matrix:\\n', cnf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_GH405yrsUZY",
        "outputId": "9c42534d-05d2-4ce6-9f33-976f9a9a6739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2942 images belonging to 3 classes.\n",
            "Found 737 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"WheatClassifier_VIT_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " LocalizationNetwork (Functiona  (None, 4)           4353604     ['input_6[0][0]']                \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, None, None,   0           ['input_6[0][0]',                \n",
            "                                3)                                'LocalizationNetwork[0][0]']    \n",
            "                                                                                                  \n",
            " patches (Patches)              (None, None, 300)    0           ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncoder)   (None, 100, 64)      25664       ['patches[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 100, 64)     128         ['patch_encoder[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 100, 64)     66368       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 100, 64)      0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'patch_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 100, 64)     128         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 100, 128)     8320        ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 100, 64)      8256        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 100, 64)      0           ['dense_8[0][0]',                \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 100, 64)     128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 100, 64)     66368       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 100, 64)      0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 100, 64)     128         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 100, 128)     8320        ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 100, 64)      8256        ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 100, 64)      0           ['dense_10[0][0]',               \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 100, 64)     128         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 6400)         0           ['layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 50)           320050      ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 50)           2550        ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 3)            153         ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,868,549\n",
            "Trainable params: 4,868,549\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_4/kernel:0', 'conv2d_4/bias:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_4/kernel:0', 'conv2d_4/bias:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node 'WheatClassifier_VIT_3/lambda_2/map/while/crop_to_bounding_box/Assert_2/Assert' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-16-a6d5bf3f7ef9>\", line 97, in <cell line: 97>\n      main()\n    File \"<ipython-input-16-a6d5bf3f7ef9>\", line 56, in main\n      results = model.fit(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/layers/core/lambda_layer.py\", line 196, in call\n      result = self.function(inputs, **kwargs)\n    File \"<ipython-input-13-deba9ae5aa18>\", line 38, in <lambda>\n      localized_inputs = layers.Lambda(lambda x: crop_and_resize(x[0], x[1]))([inputs, bbox])\n    File \"<ipython-input-13-deba9ae5aa18>\", line 23, in crop_and_resize\n      cropped_images = tf.map_fn(\n    File \"<ipython-input-13-deba9ae5aa18>\", line 24, in \n      lambda img_and_box: tf.image.crop_to_bounding_box(\n    File \"<ipython-input-13-deba9ae5aa18>\", line 24, in \n      lambda img_and_box: tf.image.crop_to_bounding_box(\nNode: 'WheatClassifier_VIT_3/lambda_2/map/while/crop_to_bounding_box/Assert_2/Assert'\nassertion failed: [target_width must be > 0.]\n\t [[{{node WheatClassifier_VIT_3/lambda_2/map/while/crop_to_bounding_box/Assert_2/Assert}}]] [Op:__inference_train_function_5792]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a6d5bf3f7ef9>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Run main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-a6d5bf3f7ef9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     results = model.fit(\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'WheatClassifier_VIT_3/lambda_2/map/while/crop_to_bounding_box/Assert_2/Assert' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-16-a6d5bf3f7ef9>\", line 97, in <cell line: 97>\n      main()\n    File \"<ipython-input-16-a6d5bf3f7ef9>\", line 56, in main\n      results = model.fit(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/layers/core/lambda_layer.py\", line 196, in call\n      result = self.function(inputs, **kwargs)\n    File \"<ipython-input-13-deba9ae5aa18>\", line 38, in <lambda>\n      localized_inputs = layers.Lambda(lambda x: crop_and_resize(x[0], x[1]))([inputs, bbox])\n    File \"<ipython-input-13-deba9ae5aa18>\", line 23, in crop_and_resize\n      cropped_images = tf.map_fn(\n    File \"<ipython-input-13-deba9ae5aa18>\", line 24, in \n      lambda img_and_box: tf.image.crop_to_bounding_box(\n    File \"<ipython-input-13-deba9ae5aa18>\", line 24, in \n      lambda img_and_box: tf.image.crop_to_bounding_box(\nNode: 'WheatClassifier_VIT_3/lambda_2/map/while/crop_to_bounding_box/Assert_2/Assert'\nassertion failed: [target_width must be > 0.]\n\t [[{{node WheatClassifier_VIT_3/lambda_2/map/while/crop_to_bounding_box/Assert_2/Assert}}]] [Op:__inference_train_function_5792]"
          ]
        }
      ],
      "source": [
        "epochs = 1                # Number of total epochs\n",
        "init_epoch = 0              # Initial epoch\n",
        "train_dir = '/content/train/'  # Training data folder path\n",
        "val_dir = '/content/val/'      # Validation data folder path\n",
        "model_id = 3               # Model ID: it can be 1, 2, 3, or 4\n",
        "load_model = 0              # If 1, load a previously trained model\n",
        "load_path = None            # Path to the pre-trained model\n",
        "backup_path = '/content/'   # Path to store the model\n",
        "batch_size = 16             # Batch size for training\n",
        "mode = 'categorical'        # Classification mode\n",
        "target_size = 100           # Size of the input images\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/plant/train'\n",
        "val_dir = '/content/drive/MyDrive/plant/val'\n",
        "\n",
        "# Define the main training function\n",
        "def main():\n",
        "\n",
        "    # Generate training and validation datasets\n",
        "    train_gen, val_gen = gen_maker(\n",
        "        train_dir,\n",
        "        val_dir,\n",
        "        target_size=(target_size, target_size),\n",
        "        batch_size=batch_size,\n",
        "        mode=mode\n",
        "    )\n",
        "\n",
        "    # Custom callback\n",
        "    clbk = CustomCallback(val_gen, backup_path, model_id)\n",
        "\n",
        "    # Set learning rate and weight decay\n",
        "    learning_rate = 0.001\n",
        "    weight_decay = 0.0001\n",
        "\n",
        "    # Create the model\n",
        "    model = model_maker((target_size, target_size), model_id)\n",
        "\n",
        "    # Set up optimizer\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=categorical_crossentropy,\n",
        "        metrics=['acc']\n",
        "    )\n",
        "\n",
        "    # Load pre-trained model if specified\n",
        "    if load_model:\n",
        "        model = tf.keras.models.load_model(load_path)\n",
        "\n",
        "    # Train the model\n",
        "    results = model.fit(\n",
        "        train_gen,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[clbk],\n",
        "        initial_epoch=init_epoch\n",
        "    )\n",
        "\n",
        "    # Predict validation data\n",
        "    y_pred_valid = model.predict(val_gen)\n",
        "\n",
        "    # Save history and model\n",
        "    history = {\n",
        "        'train loss': results.history['loss'],\n",
        "        'val loss': results.history['val_loss'],\n",
        "        'train acc': results.history['acc'],\n",
        "        'val acc': results.history['val_acc'],\n",
        "        'y_true_valid': val_gen.classes,\n",
        "        'y_pred_valid': y_pred_valid,\n",
        "        'id': model_id\n",
        "    }\n",
        "    model.save(f\"{backup_path}model.h5\")\n",
        "\n",
        "    with open(f\"{backup_path}history-id-{model_id}.pkl\", 'wb') as f:\n",
        "        pkl.dump(history, f)\n",
        "\n",
        "    # Plot training and validation metrics\n",
        "    plt.subplots(figsize=(15, 15))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(results.history['loss'], '-', color=[0, 0, 1, 1])\n",
        "    plt.plot(results.history['val_loss'], '-', color=[1, 0, 0, 1])\n",
        "    plt.legend(['train loss', 'val loss'])\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot([0, *results.history['acc']], '-', color=[0, 0, 1, 1])\n",
        "    plt.plot([0, *results.history['val_acc']], '-', color=[1, 0, 0, 1])\n",
        "    plt.legend(['train acc', 'val acc'])\n",
        "\n",
        "    plt.savefig(f\"{backup_path}charts.png\")\n",
        "\n",
        "# Run main function\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "NQbdutWIsUZY",
        "outputId": "6c143a04-820c-46f9-8592-0e9f7369438a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unknown layer: Patches. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-40059b048be9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the model with the custom optimizer specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'AdamW'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load and preprocess a single image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;34mf'Unknown {printable_module_name}: {class_name}. Please ensure this '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;34m'object is passed to the `custom_objects` argument. See '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown layer: Patches. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
          ]
        }
      ],
      "source": [
        "# EVALUATE THE MODEL --DONT USE THIS FOR MODEL 3,4 CHANGED CODE\n",
        "\n",
        "# Load the model with the custom optimizer specified\n",
        "model = tf.keras.models.load_model('/content/model.h5', custom_objects={'AdamW': AdamW})\n",
        "\n",
        "# Load and preprocess a single image\n",
        "img_path = '/content/drive/MyDrive/plant/val/Brown_rust/Brown_rust036.jpg'\n",
        "img = image.load_img(img_path, target_size=(100, 100))  # Resize as per model input\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "img_array = img_array / 255.0  # Normalize if necessary\n",
        "\n",
        "# Predict the class of the image\n",
        "predictions = model.predict(img_array)\n",
        "predicted_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"Predicted class:\", predicted_class)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model with the custom optimizer and custom layer specified\n",
        "# Load the model with the custom optimizer and custom layers specified\n",
        "model = tf.keras.models.load_model(\n",
        "    '/content/model.h5',\n",
        "    custom_objects={\n",
        "        'AdamW': AdamW,\n",
        "        'Patches': Patches,\n",
        "        'PatchEncoder': PatchEncoder  # Add PatchEncoder here\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# Load and preprocess a single image\n",
        "img_path = '/content/drive/MyDrive/plant/val/Brown_rust/Brown_rust036.jpg'\n",
        "img_path = '/content/drive/MyDrive/plant/val/Yellow_rust/Yellow_rust486.jpg'\n",
        "img_path = '/content/drive/MyDrive/plant/val/Healthy/Healthy1030.jpg'\n",
        "img = image.load_img(img_path, target_size=(100, 100))  # Resize as per model input\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "img_array = img_array / 255.0  # Normalize if necessary\n",
        "\n",
        "# Predict the class of the image\n",
        "predictions = model.predict(img_array)\n",
        "predicted_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"Predicted class:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW3fBfAdC1Jp",
        "outputId": "5d430862-a190-48f7-c881-bb185e8652d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQoIc8VBsUZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10de43d-ec03-46ed-decd-b22251ed2dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-30 16:50:36.770680: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "Found 2942 images belonging to 3 classes.\n",
            "Found 737 images belonging to 3 classes.\n",
            "2024-10-30 16:50:41.270187: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-10-30 16:50:41.270235: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Model: \"WheatClassifier_CNN_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 100, 100, 3)]     0         \n",
            "                                                                 \n",
            " conv_1 (Conv2D)             (None, 98, 98, 10)        280       \n",
            "                                                                 \n",
            " conv_2 (Conv2D)             (None, 96, 96, 10)        910       \n",
            "                                                                 \n",
            " maxpool_1 (MaxPooling2D)    (None, 48, 48, 10)        0         \n",
            "                                                                 \n",
            " conv_3 (Conv2D)             (None, 46, 46, 16)        1456      \n",
            "                                                                 \n",
            " conv_4 (Conv2D)             (None, 44, 44, 16)        2320      \n",
            "                                                                 \n",
            " maxpool_2 (MaxPooling2D)    (None, 22, 22, 16)        0         \n",
            "                                                                 \n",
            " flatten_layer (Flatten)     (None, 7744)              0         \n",
            "                                                                 \n",
            " FC_1 (Dense)                (None, 50)                387250    \n",
            "                                                                 \n",
            " leaky_ReLu_1 (LeakyReLU)    (None, 50)                0         \n",
            "                                                                 \n",
            " FC_2 (Dense)                (None, 50)                2550      \n",
            "                                                                 \n",
            " leaky_ReLu_2 (LeakyReLU)    (None, 50)                0         \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 3)                 153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 394,919\n",
            "Trainable params: 394,919\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/try.py\", line 245, in <module>\n",
            "    main()\n",
            "  File \"/content/try.py\", line 204, in main\n",
            "    results = model.fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
            "    raise e.ag_error_metadata.to_exception(e)\n",
            "ValueError: in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
            "        outputs = model.train_step(data)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 860, in train_step\n",
            "        loss = self.compute_loss(x, y, y_pred, sample_weight)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 918, in compute_loss\n",
            "        return self.compiled_loss(\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n",
            "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 141, in __call__\n",
            "        losses = call_fn(y_true, y_pred)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 245, in call  **\n",
            "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1789, in categorical_crossentropy\n",
            "        return backend.categorical_crossentropy(\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n",
            "        target.shape.assert_is_compatible_with(output.shape)\n",
            "\n",
            "    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python try.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AUZSeSgsUZY"
      },
      "outputs": [],
      "source": [
        "# WARNING:tensorflow:5 out of the last 98 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ef1d5c2dab0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
        "# Predicted class: [2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python plantdisease\\ gpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgCGwvJ02KGS",
        "outputId": "a2e92fe6-7020-4181-b734-27044d7e5c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-03 17:35:00.147342: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "Found 2942 images belonging to 3 classes.\n",
            "Found 737 images belonging to 3 classes.\n",
            "2024-11-03 17:35:06.201928: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-11-03 17:35:06.201998: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Model: \"WheatClassifier_VIT_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " patches (Patches)              (None, None, 300)    0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncoder)   (None, 100, 64)      25664       ['patches[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 100, 64)     128         ['patch_encoder[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 100, 64)     66368       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 100, 64)      0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'patch_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 100, 64)     128         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 100, 128)     8320        ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 100, 64)      8256        ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 100, 64)      0           ['dense_2[0][0]',                \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 100, 64)     128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 100, 64)     66368       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 100, 64)      0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 100, 64)     128         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 100, 128)     8320        ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 100, 64)      8256        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 100, 64)      0           ['dense_4[0][0]',                \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 100, 64)     128         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 6400)         0           ['layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50)           320050      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 50)           2550        ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 3)            153         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 514,945\n",
            "Trainable params: 514,945\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "184/184 [==============================] - ETA: 0s - loss: 1.0533 - acc: 0.47932024-11-03 17:36:27.059714: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.50      0.64       226\n",
            "           1       0.64      0.99      0.77       279\n",
            "           2       0.60      0.46      0.52       232\n",
            "\n",
            "    accuracy                           0.67       737\n",
            "   macro avg       0.71      0.65      0.65       737\n",
            "weighted avg       0.70      0.67      0.65       737\n",
            "\n",
            "\n",
            "confusion matrix:\n",
            " [[114  46  66]\n",
            " [  0 275   4]\n",
            " [ 14 112 106]]\n",
            "184/184 [==============================] - 91s 444ms/step - loss: 1.0533 - acc: 0.4793 - val_loss: 0.7138 - val_acc: 0.6716\n",
            "Predicted class: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dyy9JolH2Oyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVnl8_3MG4q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changed the model_maker from GPT"
      ],
      "metadata": {
        "id": "QS1-TLkiG4xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python pd\\ transformer\\ fix\\ for\\ gpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BsS2FNLG7LU",
        "outputId": "59ca6153-140e-4f0e-e239-e74700f57eb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 16:38:47.173569: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-11-05 16:38:47.173613: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "Found 2942 images belonging to 3 classes.\n",
            "Found 737 images belonging to 3 classes.\n",
            "2024-11-05 16:38:51.786354: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-11-05 16:38:51.786417: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2024-11-05 16:38:51.786446: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5e5cf2633970): /proc/driver/nvidia/version does not exist\n",
            "2024-11-05 16:38:51.786687: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Model: \"WheatClassifier_VIT_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " patches (Patches)              (None, None, 300)    0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncoder)   (None, 100, 64)      25664       ['patches[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 100, 64)     128         ['patch_encoder[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 100, 64)     66368       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 100, 64)      0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'patch_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 100, 64)     128         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 100, 128)     8320        ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 100, 64)      8256        ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 100, 64)      0           ['dense_2[0][0]',                \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 100, 64)     128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 100, 64)     66368       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 100, 64)      0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 100, 64)     128         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 100, 128)     8320        ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 100, 64)      8256        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 100, 64)      0           ['dense_4[0][0]',                \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 100, 64)     128         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 6400)         0           ['layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50)           320050      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 50)           2550        ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 3)            153         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 514,945\n",
            "Trainable params: 514,945\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "184/184 [==============================] - ETA: 0s - loss: 1.1816 - acc: 0.36202024-11-05 16:52:29.784073: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\n",
            "classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       226\n",
            "           1       0.38      1.00      0.55       279\n",
            "           2       0.00      0.00      0.00       232\n",
            "\n",
            "    accuracy                           0.38       737\n",
            "   macro avg       0.13      0.33      0.18       737\n",
            "weighted avg       0.14      0.38      0.21       737\n",
            "\n",
            "\n",
            "confusion matrix:\n",
            " [[  0 225   1]\n",
            " [  0 279   0]\n",
            " [  0 232   0]]\n",
            "184/184 [==============================] - 821s 4s/step - loss: 1.1816 - acc: 0.3620 - val_loss: 1.1520 - val_acc: 0.3786\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/pd transformer fix for gpt.py\", line 305, in <module>\n",
            "    model = tf.keras.models.load_model('/content/model.h5', custom_objects={'AdamW': AdamW})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/generic_utils.py\", line 562, in class_and_config_for_serialized_keras_object\n",
            "    raise ValueError(\n",
            "ValueError: Unknown layer: Patches. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 05 - 11 - OBJECT bounding box from GPT"
      ],
      "metadata": {
        "id": "xpGNhcvMU5NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python 02fix\\ from\\ gpt.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfS7lt5JU858",
        "outputId": "0c8d2cd6-4900-4742-8f4f-d9baf7363305"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 17:24:53.472065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "Found 2942 images belonging to 3 classes.\n",
            "Found 737 images belonging to 3 classes.\n",
            "2024-11-05 17:24:58.314537: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-11-05 17:24:58.314583: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Model: \"WheatClassifier_VIT_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " patches (Patches)              (None, None, 300)    0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncoder)   (None, 100, 64)      25664       ['patches[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 100, 64)     128         ['patch_encoder[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 100, 64)     66368       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 100, 64)      0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'patch_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 100, 64)     128         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 100, 128)     8320        ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 100, 64)      8256        ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 100, 64)      0           ['dense_2[0][0]',                \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 100, 64)     128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 100, 64)     66368       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 100, 64)      0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 100, 64)     128         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 100, 128)     8320        ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 100, 64)      8256        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 100, 64)      0           ['dense_4[0][0]',                \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 100, 64)     128         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 6400)         0           ['layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50)           320050      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 50)           320050      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 50)           2550        ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 50)           2550        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " class_output (Dense)           (None, 3)            153         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " bbox_output (Dense)            (None, 4)            204         ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 837,749\n",
            "Trainable params: 837,749\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/02fix from gpt.py\", line 410, in <module>\n",
            "    main_localization()\n",
            "  File \"/content/02fix from gpt.py\", line 398, in main_localization\n",
            "    results = model.fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\", line 54, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\n",
            "\n",
            "Detected at node 'gradient_tape/mean_squared_error/BroadcastGradientArgs' defined at (most recent call last):\n",
            "    File \"/content/02fix from gpt.py\", line 410, in <module>\n",
            "      main_localization()\n",
            "    File \"/content/02fix from gpt.py\", line 398, in main_localization\n",
            "      results = model.fit(\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n",
            "      return fn(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1384, in fit\n",
            "      tmp_logs = self.train_function(iterator)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1021, in train_function\n",
            "      return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1010, in step_function\n",
            "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1000, in run_step\n",
            "      outputs = model.train_step(data)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 863, in train_step\n",
            "      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py\", line 163, in minimize\n",
            "      return super().minimize(\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n",
            "      grads_and_vars = self._compute_gradients(\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n",
            "      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n",
            "      grads = tape.gradient(loss, var_list, grad_loss)\n",
            "Node: 'gradient_tape/mean_squared_error/BroadcastGradientArgs'\n",
            "Incompatible shapes: [16,4] vs. [16,3]\n",
            "\t [[{{node gradient_tape/mean_squared_error/BroadcastGradientArgs}}]] [Op:__inference_train_function_5387]\n",
            "2024-11-05 17:25:09.062059: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: FAILED_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.\n",
            "\t [[{{node PyFunc}}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python 03claudetry1.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stfxvkjzVAe9",
        "outputId": "d18469ca-b911-4959-bd3b-01e5fb3629fc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 17:33:07.824753: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "Found 2942 images belonging to 3 classes.\n",
            "Found 737 images belonging to 3 classes.\n",
            "2024-11-05 17:33:12.370068: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-11-05 17:33:12.370113: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Model: \"PlantDiseaseClassifier_VIT_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " patches (Patches)              (None, None, 300)    0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncoder)   (None, 100, 64)      25664       ['patches[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 100, 64)     128         ['patch_encoder[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 100, 64)     66368       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 100, 64)      0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'patch_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 100, 64)     128         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 100, 128)     8320        ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 100, 64)      8256        ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 100, 64)      0           ['dense_2[0][0]',                \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 100, 64)     128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 100, 64)     66368       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 100, 64)      0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 100, 64)     128         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 100, 128)     8320        ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 100, 64)      8256        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 100, 64)      0           ['dense_4[0][0]',                \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 100, 64)     128         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 6400)         0           ['layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50)           320050      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 50)           320050      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 50)           2550        ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 50)           2550        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " classification (Dense)         (None, 3)            153         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " localization (Dense)           (None, 1)            51          ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 837,596\n",
            "Trainable params: 837,596\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 1.6273 - classification_loss: 0.9731 - localization_loss: 0.6542 - classification_acc: 0.5398 - localization_mse: 0.22562024-11-05 17:34:35.278305: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.69      0.80       226\n",
            "           1       0.88      0.67      0.76       279\n",
            "           2       0.59      0.91      0.72       232\n",
            "\n",
            "    accuracy                           0.75       737\n",
            "   macro avg       0.80      0.76      0.76       737\n",
            "weighted avg       0.81      0.75      0.76       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[156  13  57]\n",
            " [  1 188  90]\n",
            " [  8  13 211]]\n",
            "\n",
            "Localization MSE: 1.119292761509705\n",
            "184/184 [==============================] - 94s 456ms/step - loss: 1.6273 - classification_loss: 0.9731 - localization_loss: 0.6542 - classification_acc: 0.5398 - localization_mse: 0.2256 - val_loss: 1.1897 - val_classification_loss: 0.5501 - val_localization_loss: 0.6395 - val_classification_acc: 0.7531 - val_localization_mse: 0.2235\n",
            "Epoch 2/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 1.1061 - classification_loss: 0.4677 - localization_loss: 0.6384 - classification_acc: 0.8154 - localization_mse: 0.2228WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.89       226\n",
            "           1       0.96      0.77      0.86       279\n",
            "           2       0.74      0.94      0.83       232\n",
            "\n",
            "    accuracy                           0.86       737\n",
            "   macro avg       0.87      0.86      0.86       737\n",
            "weighted avg       0.87      0.86      0.86       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[198   2  26]\n",
            " [ 14 216  49]\n",
            " [  6   8 218]]\n",
            "\n",
            "Localization MSE: 1.0623004049589142\n",
            "184/184 [==============================] - 81s 441ms/step - loss: 1.1061 - classification_loss: 0.4677 - localization_loss: 0.6384 - classification_acc: 0.8154 - localization_mse: 0.2228 - val_loss: 1.0215 - val_classification_loss: 0.3843 - val_localization_loss: 0.6372 - val_classification_acc: 0.8575 - val_localization_mse: 0.2225\n",
            "Epoch 3/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 1.0439 - classification_loss: 0.4056 - localization_loss: 0.6383 - classification_acc: 0.8491 - localization_mse: 0.2229WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.88      0.92       226\n",
            "           1       0.78      0.99      0.87       279\n",
            "           2       0.96      0.74      0.83       232\n",
            "\n",
            "    accuracy                           0.88       737\n",
            "   macro avg       0.90      0.87      0.88       737\n",
            "weighted avg       0.89      0.88      0.88       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[200  21   5]\n",
            " [  2 275   2]\n",
            " [  6  55 171]]\n",
            "\n",
            "Localization MSE: 1.0525090667665928\n",
            "184/184 [==============================] - 82s 445ms/step - loss: 1.0439 - classification_loss: 0.4056 - localization_loss: 0.6383 - classification_acc: 0.8491 - localization_mse: 0.2229 - val_loss: 0.9591 - val_classification_loss: 0.3216 - val_localization_loss: 0.6375 - val_classification_acc: 0.8765 - val_localization_mse: 0.2227\n",
            "Epoch 4/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.8939 - classification_loss: 0.2570 - localization_loss: 0.6369 - classification_acc: 0.9011 - localization_mse: 0.2224WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94       226\n",
            "           1       0.93      0.99      0.96       279\n",
            "           2       0.93      0.93      0.93       232\n",
            "\n",
            "    accuracy                           0.94       737\n",
            "   macro avg       0.95      0.94      0.94       737\n",
            "weighted avg       0.94      0.94      0.94       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[203   8  15]\n",
            " [  2 276   1]\n",
            " [  3  13 216]]\n",
            "\n",
            "Localization MSE: 1.08471581912526\n",
            "184/184 [==============================] - 82s 444ms/step - loss: 0.8939 - classification_loss: 0.2570 - localization_loss: 0.6369 - classification_acc: 0.9011 - localization_mse: 0.2224 - val_loss: 0.7829 - val_classification_loss: 0.1462 - val_localization_loss: 0.6368 - val_classification_acc: 0.9430 - val_localization_mse: 0.2223\n",
            "Epoch 5/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.8372 - classification_loss: 0.2005 - localization_loss: 0.6367 - classification_acc: 0.9296 - localization_mse: 0.2223WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.82      0.90       226\n",
            "           1       0.95      0.97      0.96       279\n",
            "           2       0.87      0.99      0.93       232\n",
            "\n",
            "    accuracy                           0.93       737\n",
            "   macro avg       0.94      0.93      0.93       737\n",
            "weighted avg       0.94      0.93      0.93       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[186  13  27]\n",
            " [  1 272   6]\n",
            " [  0   2 230]]\n",
            "\n",
            "Localization MSE: 1.081595236784326\n",
            "184/184 [==============================] - 80s 437ms/step - loss: 0.8372 - classification_loss: 0.2005 - localization_loss: 0.6367 - classification_acc: 0.9296 - localization_mse: 0.2223 - val_loss: 0.8126 - val_classification_loss: 0.1759 - val_localization_loss: 0.6366 - val_classification_acc: 0.9335 - val_localization_mse: 0.2223\n",
            "Epoch 6/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.8264 - classification_loss: 0.1897 - localization_loss: 0.6367 - classification_acc: 0.9300 - localization_mse: 0.2223WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96       226\n",
            "           1       0.96      1.00      0.98       279\n",
            "           2       0.96      0.97      0.96       232\n",
            "\n",
            "    accuracy                           0.97       737\n",
            "   macro avg       0.97      0.97      0.97       737\n",
            "weighted avg       0.97      0.97      0.97       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[211   6   9]\n",
            " [  0 278   1]\n",
            " [  2   5 225]]\n",
            "\n",
            "Localization MSE: 1.0851551051706008\n",
            "184/184 [==============================] - 80s 432ms/step - loss: 0.8264 - classification_loss: 0.1897 - localization_loss: 0.6367 - classification_acc: 0.9300 - localization_mse: 0.2223 - val_loss: 0.7295 - val_classification_loss: 0.0928 - val_localization_loss: 0.6367 - val_classification_acc: 0.9688 - val_localization_mse: 0.2223\n",
            "Epoch 7/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.8421 - classification_loss: 0.2053 - localization_loss: 0.6368 - classification_acc: 0.9249 - localization_mse: 0.2224WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95       226\n",
            "           1       0.96      0.96      0.96       279\n",
            "           2       0.93      0.97      0.95       232\n",
            "\n",
            "    accuracy                           0.95       737\n",
            "   macro avg       0.95      0.95      0.95       737\n",
            "weighted avg       0.95      0.95      0.95       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[211   8   7]\n",
            " [  2 268   9]\n",
            " [  5   3 224]]\n",
            "\n",
            "Localization MSE: 1.0745390788050937\n",
            "184/184 [==============================] - 79s 427ms/step - loss: 0.8421 - classification_loss: 0.2053 - localization_loss: 0.6368 - classification_acc: 0.9249 - localization_mse: 0.2224 - val_loss: 0.7787 - val_classification_loss: 0.1416 - val_localization_loss: 0.6371 - val_classification_acc: 0.9539 - val_localization_mse: 0.2225\n",
            "Epoch 8/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.8333 - classification_loss: 0.1966 - localization_loss: 0.6367 - classification_acc: 0.9310 - localization_mse: 0.2223WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.94      0.95       226\n",
            "           1       0.92      1.00      0.96       279\n",
            "           2       0.99      0.91      0.95       232\n",
            "\n",
            "    accuracy                           0.95       737\n",
            "   macro avg       0.96      0.95      0.95       737\n",
            "weighted avg       0.95      0.95      0.95       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[212  12   2]\n",
            " [  0 279   0]\n",
            " [  8  13 211]]\n",
            "\n",
            "Localization MSE: 1.0769995111261905\n",
            "184/184 [==============================] - 82s 444ms/step - loss: 0.8333 - classification_loss: 0.1966 - localization_loss: 0.6367 - classification_acc: 0.9310 - localization_mse: 0.2223 - val_loss: 0.7870 - val_classification_loss: 0.1504 - val_localization_loss: 0.6366 - val_classification_acc: 0.9525 - val_localization_mse: 0.2222\n",
            "Epoch 9/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.8207 - classification_loss: 0.1840 - localization_loss: 0.6366 - classification_acc: 0.9290 - localization_mse: 0.2223WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95       226\n",
            "           1       0.92      1.00      0.96       279\n",
            "           2       0.96      0.92      0.94       232\n",
            "\n",
            "    accuracy                           0.95       737\n",
            "   macro avg       0.96      0.95      0.95       737\n",
            "weighted avg       0.95      0.95      0.95       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[210   8   8]\n",
            " [  1 278   0]\n",
            " [  3  16 213]]\n",
            "\n",
            "Localization MSE: 1.0689094231039855\n",
            "184/184 [==============================] - 80s 437ms/step - loss: 0.8207 - classification_loss: 0.1840 - localization_loss: 0.6366 - classification_acc: 0.9290 - localization_mse: 0.2223 - val_loss: 0.7707 - val_classification_loss: 0.1341 - val_localization_loss: 0.6366 - val_classification_acc: 0.9512 - val_localization_mse: 0.2223\n",
            "Epoch 10/10\n",
            "184/184 [==============================] - ETA: 0s - loss: 0.7863 - classification_loss: 0.1497 - localization_loss: 0.6366 - classification_acc: 0.9480 - localization_mse: 0.2223WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.88      0.93       226\n",
            "           1       0.99      0.97      0.98       279\n",
            "           2       0.89      1.00      0.94       232\n",
            "\n",
            "    accuracy                           0.95       737\n",
            "   macro avg       0.96      0.95      0.95       737\n",
            "weighted avg       0.96      0.95      0.95       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[199   3  24]\n",
            " [  1 272   6]\n",
            " [  0   1 231]]\n",
            "\n",
            "Localization MSE: 1.0769405734862367\n",
            "184/184 [==============================] - 78s 426ms/step - loss: 0.7863 - classification_loss: 0.1497 - localization_loss: 0.6366 - classification_acc: 0.9480 - localization_mse: 0.2223 - val_loss: 0.7605 - val_classification_loss: 0.1239 - val_localization_loss: 0.6366 - val_classification_acc: 0.9525 - val_localization_mse: 0.2223\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/03claudetry1.py\", line 298, in <module>\n",
            "    main(include_localization=True)\n",
            "  File \"/content/03claudetry1.py\", line 268, in main\n",
            "    'train acc': results.history['acc'],\n",
            "KeyError: 'acc'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python 03claudetry1.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF4yG_J7Yfoz",
        "outputId": "857e1294-b515-40f4-9d28-fe356f247061"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 17:54:18.549596: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "Found 2942 images belonging to 3 classes.\n",
            "Found 737 images belonging to 3 classes.\n",
            "2024-11-05 17:54:24.900677: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2024-11-05 17:54:24.900725: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Model: \"PlantDiseaseClassifier_VIT_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " patches (Patches)              (None, None, 300)    0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncoder)   (None, 100, 64)      25664       ['patches[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 100, 64)     128         ['patch_encoder[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 100, 64)     66368       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 100, 64)      0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'patch_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 100, 64)     128         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 100, 128)     8320        ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 100, 64)      8256        ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 100, 64)      0           ['dense_2[0][0]',                \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 100, 64)     128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 100, 64)     66368       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 100, 64)      0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 100, 64)     128         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 100, 128)     8320        ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 100, 64)      8256        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 100, 64)      0           ['dense_4[0][0]',                \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 100, 64)     128         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 6400)         0           ['layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50)           320050      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 50)           320050      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 50)           2550        ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 50)           2550        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " classification (Dense)         (None, 3)            153         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " localization (Dense)           (None, 1)            51          ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 837,596\n",
            "Trainable params: 837,596\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "184/184 [==============================] - ETA: 0s - loss: 1.7258 - classification_loss: 1.0553 - localization_loss: 0.6704 - classification_acc: 0.4963 - localization_mse: 0.22682024-11-05 17:55:50.929904: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 28). These functions will not be directly callable after loading.\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.58      0.73       226\n",
            "           1       0.64      0.99      0.77       279\n",
            "           2       0.73      0.53      0.61       232\n",
            "\n",
            "    accuracy                           0.72       737\n",
            "   macro avg       0.77      0.70      0.70       737\n",
            "weighted avg       0.76      0.72      0.71       737\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[132  52  42]\n",
            " [  0 275   4]\n",
            " [  6 104 122]]\n",
            "\n",
            "Localization MSE: 1.0743238307456973\n",
            "184/184 [==============================] - 97s 475ms/step - loss: 1.7258 - classification_loss: 1.0553 - localization_loss: 0.6704 - classification_acc: 0.4963 - localization_mse: 0.2268 - val_loss: 1.4228 - val_classification_loss: 0.7860 - val_localization_loss: 0.6368 - val_classification_acc: 0.7178 - val_localization_mse: 0.2223\n",
            "Saved\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/03claudetry1.py\", line 301, in <module>\n",
            "    model = tf.keras.models.load_model('/content/model.h5', custom_objects={'AdamW': AdamW})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/generic_utils.py\", line 562, in class_and_config_for_serialized_keras_object\n",
            "    raise ValueError(\n",
            "ValueError: Unknown layer: Patches. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(\n",
        "    '/content/model.h5',\n",
        "    custom_objects={\n",
        "        'AdamW': AdamW,\n",
        "        'Patches': Patches,\n",
        "        'PatchEncoder': PatchEncoder  # Add PatchEncoder here\n",
        "    }\n",
        ")\n",
        "\n",
        "img_path = '/content/drive/MyDrive/plant/val/Healthy/Healthy1030.jpg'\n",
        "img_path = '/content/drive/MyDrive/plant/val/Yellow_rust/Yellow_rust486.jpg'\n",
        "img_path = '/content/drive/MyDrive/plant/val/Brown_rust/Brown_rust036.jpg'\n",
        "img = image.load_img(img_path, target_size=(100, 100))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0\n",
        "\n",
        "class_preds, loc_preds = model.predict(img_array)\n",
        "print(\"Predicted class:\", np.argmax(class_preds))\n",
        "print(\"Localization prediction:\", loc_preds[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwfTk8ptcO9q",
        "outputId": "19ff09ff-f2c9-4602-b048-f52ab620426b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n",
            "Localization prediction: 0.35717332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def draw_bounding_boxes(image, boxes, scores, labels):\n",
        "    \"\"\"\n",
        "    Draw bounding boxes on the given image.\n",
        "    \"\"\"\n",
        "    img = image.copy()\n",
        "    # for box, score, label in zip(boxes, scores, labels):\n",
        "    #     print(box)\n",
        "    #     x1, y1, x2, y2 = [int(x) for x in box]\n",
        "    #     cv2.rectangle(img, (x1, y1), (x2, y2), (36, 255, 12), 2)\n",
        "    #     cv2.putText(img, f\"{label}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36, 255, 12), 2)\n",
        "    # make boxes as int array\n",
        "    boxes = np.array(boxes).astype(int)\n",
        "    x1 = boxes[0]\n",
        "    y1 = boxes[1]\n",
        "    x2 = boxes[2]\n",
        "    y2 = boxes[3]\n",
        "    score = scores[0]\n",
        "    label = labels[0]\n",
        "    print(x1, y1, x2, y2, score, label)\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), (36, 255, 12), 2)\n",
        "    # cv2.putText(img, f\"{label}: {score:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36, 255, 12), 2)\n",
        "    return img\n",
        "boxes = loc_preds[0] * [100, 100, 100, 100]  # Rescale the bounding box coordinates\n",
        "boxes = [b for b in boxes]\n",
        "print(boxes)\n",
        "labels = ['Disease']\n",
        "scores = [class_preds[0, np.argmax(class_preds)]]\n",
        "# convert img to numpy array\n",
        "img = np.array(img)\n",
        "img_with_boxes = draw_bounding_boxes(img, boxes, scores, labels)\n",
        "plt.imshow(img_with_boxes)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "Sg201ZmRe9NE",
        "outputId": "e825b12b-cdf3-44ab-8951-3787e116de4e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[35.71733236312866, 35.71733236312866, 35.71733236312866, 35.71733236312866]\n",
            "35 35 35 35 0.8121615 Disease\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}